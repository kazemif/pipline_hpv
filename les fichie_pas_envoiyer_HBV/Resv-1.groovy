// Activer DSL2 de Nextflow
//nextflow.enable.dsl=2

// Modules imports
//include { merge_fastq } from "./modules/merge_fastq"
//include { trim_reads } from './modules/trim_reads.nf'
//include { parse_fastq_qc } from './modules/parse_fastq_qc.nf'
//include { mapping_bam } from "./modules/mapping_bam.nf"
//include { parse_bam_qc } from './modules/parse_bam_qc.nf'
//include { bam_indexing } from "./modules/bam_indexing.nf"
//include { bam_coverage } from './modules/bam_coverage.nf'
//include { low_coverage_filtering } from './modules/low_coverage_filtering.nf'
//include { variant_calling } from './modules/variant_calling.nf'
//include { parse_variant_qc } from './modules/parse_variant_qc.nf'
//include { consensus_generation } from './modules/consensus_generation.nf'



// Definition des parametres globaux
//params.fastq_dir = "/home/etudiant/fatemeh/hbv_pipeline/data_test"  // Fichiers FASTQ

//params.ref_genome = "/home/etudiant/fatemeh/hbv_pipeline/Ref/sequence.fasta"  // Reference genomique
//params.result_dir = "/home/etudiant/fatemeh/hbv_pipeline/result/"  // Repertoire de sortie


//Adapters
//params.adapter = "/home/etudiant/fatemeh/hbv_pipeline/primer/HBV_primer.fasta"

//Trimming option
//params.min_len = 150
//params.max_len = 1200
//params.min_qual = 10

//Computing option
//params.cpu = 10


         

//workflow {
    // Charger les fichiers FASTQ
  //  fastq_ch = Channel.fromPath("${params.fastq_dir}/*.fastq.gz").collect()

    // √âtapes de pr√©traitement
    //merged_fastq_ch = merge_fastq(fastq_ch, params.sample_name)
    //trimmed_fastq_ch = trim_reads(merged_fastq_ch, params.adapter, params.min_len, params.max_len, params.min_qual, params.cpu)

     // √âtape 1 : Analyse de qualit√© des fichiers FASTQ**********************
    //parce_fastq_qc_ch = parse_fastq_qc(fastq_ch)  parse_fastq_qc

    // √âtape  2 : Mapping avec Minimap2 + tri BAM
    //mapped_bam_ch = mapping_bam(trimmed_fastq_ch, params.ref_genome)

    // √âtape 3 : Analyse des fichiers BAM*************************
 //parse_bam_qc = parse_bam_qc _bam_qc(mapped_bam_ch)

    // √âtape 4 : Indexation du BAM (obligatoire pour appel de variants)
    //bam_outputs = bam_indexing(mapped_bam_ch)
    //indexed_bam_ch = bam_outputs[0]
    ///indexed_bai_ch = bam_outputs[1]

    // √âtape 5 : Calcul de la couverture (utilise BAM tri√©, PAS index√©)
    //coverage_bed_ch = bam_coverage(mapped_bam_ch, params.ref_genome)  // G√©n√®re `.coverage.bed`

    // √âtape 6 : Filtrage des r√©gions √† faible couverture
    //low_coverage_bed_ch = low_coverage_filtering(coverage_bed_ch)  // Prend `.coverage.bed` en entr√©e

    // √âtape 7 : Variant calling + compression + indexation
    //vcf_outputs = variant_calling(indexed_bam_ch, indexed_bai_ch, params.ref_genome)
    //vcf_gz_ch = vcf_outputs[0]  // `.vcf.gz`
    //vcf_tbi_ch = vcf_outputs[1] // `.vcf.gz.tbi`

    //√âtape 8 : Analyse des variants*************************************
    //parse_variant_qc_ch = parse_variant_qc(vcf_gz_ch, indexed_bam_ch, params.ref_genome)

    // √âtape 9 : G√©n√©ration du consensus
    //consensus_fasta_ch = consensus_generation(vcf_gz_ch, vcf_tbi_ch, params.ref_genome, low_coverage_bed_ch)
//}








//nextflow.enable.dsl=2

// ===== Modules imports =====
//include { merge_fastq }            from './modules/merge_fastq.nf'
//include { trim_reads }             from './modules/trim_reads.nf'
//include { parse_fastq_qc }         from './modules/parse_fastq_qc.nf'
//include { mapping_bam }            from './modules/mapping_bam.nf'
//include { parse_bam_qc }           from './modules/parse_bam_qc.nf'
//include { bam_indexing }           from './modules/bam_indexing.nf'
//include { bam_coverage }           from './modules/bam_coverage.nf'
//include { low_coverage_filtering } from './modules/low_coverage_filtering.nf'
//include { variant_calling }        from './modules/variant_calling.nf'
//include { parse_variant_qc }      from './modules/parse_variant_qc.nf'
//include { consensus_generation }   from './modules/consensus_generation.nf'

// ===== Param√®tres =====
//params.fastq_dir = "/home/etudiant/fatemeh/hbv_pipeline/data_test"
//params.result_dir = "/home/etudiant/fatemeh/hbv_pipeline/result/"
//params.ref_genome = "/home/etudiant/fatemeh/hbv_pipeline/Ref/sequence.fasta"
//params.adapter = "/home/etudiant/fatemeh/hbv_pipeline/primer/HBV_primer.fasta"
//params.min_len = 150
//params.max_len = 1200
//params.min_qual = 10
//params.cpu = 10

//workflow {

    // === D√©tection automatique du type d'entr√©e ===
   // if (file(params.fastq_dir).isDirectory()) {
      //  def files = file(params.fastq_dir).listFiles().findAll { it.name.endsWith('.fastq') || it.name.endsWith('.fastq.gz') }

      //  if (files.every { it.isFile() }) {
          //  println "üîπ Dossier plat d√©tect√©"
         //   fastq_ch = Channel.fromFilePairs("${params.fastq_dir}/*.{fastq,fastq.gz}", flat: true)
       // } 
       // else {
         //   println "üîπ Dossier multiplex (barcodes) d√©tect√©"
       //     fastq_ch = Channel.fromFilePairs("${params.fastq_dir}/*/*.fastq", flat: true)
     //   }
   // } 
   // else {
    //    error "‚ùå Le chemin dans params.fastq_dir n'est pas valide"
   // }

    // === √âtape 1 : Merge FASTQ ===
   // merged_fastq_ch = merge_fastq(fastq_ch)

    // === √âtape 2 : Trimming ===
   // trimmed_fastq_ch = trim_reads(
      //  merged_fastq_ch,
      //  params.adapter,
      //  params.min_len,
      //  params.max_len,
     //   params.min_qual,
     //   params.cpu
   // )

    // === √âtape 3 : Analyse qualit√© FASTQ ===
   // fastq_qc_ch = parse_fastq_qc(merged_fastq_ch)

    // === √âtape 4 : Mapping ===
   // mapped_bam_ch = mapping_bam(trimmed_fastq_ch, params.ref_genome)

    // === √âtape 5 : Analyse BAM ===
   // bam_qc_ch = parse_bam_qc(mapped_bam_ch)

    // === √âtape 6 : Indexation BAM ===
   // bam_index_outputs = bam_indexing(mapped_bam_ch)
   // indexed_bam_ch = bam_index_outputs[0]
   // indexed_bai_ch = bam_index_outputs[1]

    // === √âtape 7 : Couverture BAM ===
  //  coverage_bed_ch = bam_coverage(mapped_bam_ch, params.ref_genome)

    // === Ensuite : filtering, variant calling, etc. ===
//}












nextflow.enable.dsl=2

// ===== Modules imports =====
include { merge_fastq }            from './modules/merge_fastq.nf'
include { trim_reads }             from './modules/trim_reads.nf'
include { parse_fastq_qc }         from './modules/parse_fastq_qc.nf'
//include { mapping_bam }            from './modules/mapping_bam.nf'
//include { parse_bam_qc }           from './modules/parse_bam_qc.nf'
//include { bam_indexing }           from './modules/bam_indexing.nf'
//include { bam_coverage }           from './modules/bam_coverage.nf'
//include { low_coverage_filtering } from './modules/low_coverage_filtering.nf'
//include { variant_calling }        from './modules/variant_calling.nf'
//include { parse_variant_qc }      from './modules/parse_variant_qc.nf'
//include { consensus_generation }   from './modules/consensus_generation.nf'

// ===== Param√®tres =====
params.fastq = null
params.sample = null
params.ref_genome = "/home/etudiant/fatemeh/hbv_pipeline/Ref/sequence.fasta"
params.adapter = "/home/etudiant/fatemeh/hbv_pipeline/primer/HBV_primer.fasta"
params.min_len = 150
params.max_len = 1200
params.min_qual = 10
params.cpu = 10

workflow {

    // === D√©tection automatique du type d'entr√©e ===
    if (file(params.fastq).isFile()) {
        println "üîπ Fichier unique d√©tect√©"
        fastq_ch = Channel.of([params.sample ?: "sample1", file(params.fastq)])
    }
    else if (file(params.fastq).isDirectory()) {
        def files = file(params.fastq).listFiles().findAll { it.name.endsWith('.fastq') || it.name.endsWith('.fastq.gz') }

        if (files.every { it.isFile() }) {
            println "üîπ Dossier plat d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.fastq}/*.{fastq,fastq.gz}", flat: true)
        } 
        else {
            println "üîπ Dossier multiplex d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.fastq}/*/*.fastq", flat: true)
        }
    } 
    else {
        error "‚ùå Le chemin donn√© avec --fastq n'est pas valide"
    }

    // === √âtape 1 : Merge FASTQ ===
    merged_fastq_ch = merge_fastq(fastq_ch)      


    // === √âtape 2 : Trimming ===
    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        params.adapter,
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    // === √âtape 3 : Analyse qualit√© FASTQ ===
    fastq_qc_ch = parse_fastq_qc(merged_fastq_ch)

    // === √âtape 4 : Mapping ===
   // mapped_bam_ch = mapping_bam(trimmed_fastq_ch, params.ref_genome)

    // === √âtape 5 : Analyse BAM ===
    //bam_qc_ch = parse_bam_qc(mapped_bam_ch)

    // === √âtape 6 : Indexation BAM ===
  //  bam_index_outputs = bam_indexing(mapped_bam_ch)
//    indexed_bam_ch = bam_index_outputs[0]
//    indexed_bai_ch = bam_index_outputs[1]

    // === √âtape 7 : Couverture BAM ===
    coverage_bed_ch = bam_coverage(mapped_bam_ch, params.ref_genome)

    // === √âtape 8 : Filtering, Variant calling, QC Variant, Consensus
    filtered_bam_ch = low_coverage_filtering(coverage_bed_ch)
    variant_vcf_ch  = variant_calling(indexed_bam_ch, params.ref_genome)
    variant_qc_ch   = parse_variant_qc(variant_vcf_ch)
    consensus_fa_ch = consensus_generation(indexed_bam_ch, params.ref_genome)
}
nextflow.enable.dsl=2

// ===== Modules imports =====
include { merge_fastq }            from './modules/merge_fastq.nf'
//include { trim_reads }             from './modules/trim_reads.nf'
//include { parse_fastq_qc }         from './modules/parse_fastq_qc.nf'
//include { mapping_bam }            from './modules/mapping_bam.nf'
//include { parse_bam_qc }           from './modules/parse_bam_qc.nf'
//include { bam_indexing }           from './modules/bam_indexing.nf'
//include { bam_coverage }           from './modules/bam_coverage.nf'
//include { low_coverage_filtering } from './modules/low_coverage_filtering.nf'
//include { variant_calling }        from './modules/variant_calling.nf'
//include { parse_variant_qc }      from './modules/parse_variant_qc.nf'
//include { consensus_generation }   from './modules/consensus_generation.nf'

// ===== Param√®tres =====
params.input = null
params.sample = null
params.ref_genome = "/home/etudiant/fatemeh/hbv_pipeline/Ref/sequence.fasta"
params.adapter = "/home/etudiant/fatemeh/hbv_pipeline/primer/HBV_primer.fasta"
params.min_len = 150
params.max_len = 1200
params.min_qual = 10
params.cpu = 10

workflow {

    if (params.input == null) {
        error "‚ùå Vous devez donner un chemin avec --input"
    }

    // D√©tection du type d'entr√©e
    if (file(params.input).isFile()) {
        if (params.input.endsWith('.fastq') || params.input.endsWith('.fastq.gz')) {
            println "üîπ Fichier FASTQ d√©tect√©"
            fastq_ch = Channel.of([params.sample ?: "sample1", file(params.input)])
        } else if (params.input.endsWith('.fasta')) {
            println "üîπ Fichier FASTA d√©tect√©"
            fasta_ch = Channel.of([params.sample ?: "sample1", file(params.input)])
        } else if (params.input.endsWith('.csv')) {
            println "üîπ Fichier CSV d√©tect√©"
            csv_ch = Channel.of(file(params.input))
        } else if (params.input.endsWith('.tar.gz')) {
            error "‚ùóÔ∏è Archive d√©tect√©e. Veuillez la d√©compresser avant de lancer le pipeline."
        } else {
            error "‚ùå Format de fichier non reconnu."
        }

    } else if (file(params.input).isDirectory()) {
        def files = file(params.input).listFiles()

        if (files.any { it.isDirectory() }) {
            println "üîπ Dossier multiplex d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.input}/*/*.fastq", flat: true)
            fasta_ch = Channel.fromFilePairs("${params.input}/*/*.fasta", flat: true)
        } else if (files.any { it.name.endsWith('.fastq') || it.name.endsWith('.fastq.gz') }) {
            println "üîπ Dossier plat FASTQ d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.input}/*.{fastq,fastq.gz}", flat: true)
        } else if (files.any { it.name.endsWith('.fasta') }) {
            println "üîπ Dossier plat FASTA d√©tect√©"
            fasta_ch = Channel.fromFilePairs("${params.input}/*.fasta", flat: true)
        } else {
            error "‚ùå Aucun fichier FASTQ, FASTA ou CSV trouv√©."
        }
    } else {
        error "‚ùå Le chemin donn√© avec --input n'est pas valide"
    }

    // Charger les fichiers FASTQ
  //  fastq_ch = Channel.fromPath("${params.fastq_dir}/*.fastq.gz").collect()

    // √âtapes de pr√©traitement
    //merged_fastq_ch = merge_fastq(fastq_ch, params.sample_name)
    //trimmed_fastq_ch = trim_reads(merged_fastq_ch, params.adapter, params.min_len, params.max_len, params.min_qual, params.cpu)

     // √âtape 1 : Analyse de qualit√© des fichiers FASTQ**********************
    //parce_fastq_qc_ch = parse_fastq_qc(fastq_ch)  parse_fastq_qc

    // √âtape  2 : Mapping avec Minimap2 + tri BAM
    //mapped_bam_ch = mapping_bam(trimmed_fastq_ch, params.ref_genome)

    // √âtape 3 : Analyse des fichiers BAM*************************
 //parse_bam_qc = parse_bam_qc _bam_qc(mapped_bam_ch)

    // √âtape 4 : Indexation du BAM (obligatoire pour appel de variants)
    //bam_outputs = bam_indexing(mapped_bam_ch)
    //indexed_bam_ch = bam_outputs[0]
    ///indexed_bai_ch = bam_outputs[1]

    // √âtape 5 : Calcul de la couverture (utilise BAM tri√©, PAS index√©)
    //coverage_bed_ch = bam_coverage(mapped_bam_ch, params.ref_genome)  // G√©n√®re `.coverage.bed`

    // √âtape 6 : Filtrage des r√©gions √† faible couverture
    //low_coverage_bed_ch = low_coverage_filtering(coverage_bed_ch)  // Prend `.coverage.bed` en entr√©e

    // √âtape 7 : Variant calling + compression + indexation
    //vcf_outputs = variant_calling(indexed_bam_ch, indexed_bai_ch, params.ref_genome)
    //vcf_gz_ch = vcf_outputs[0]  // `.vcf.gz`
    //vcf_tbi_ch = vcf_outputs[1] // `.vcf.gz.tbi`

    //√âtape 8 : Analyse des variants*************************************
    //parse_variant_qc_ch = parse_variant_qc(vcf_gz_ch, indexed_bam_ch, params.ref_genome)

    // √âtape 9 : G√©n√©ration du consensus
    //consensus_fasta_ch = consensus_generation(vcf_gz_ch, vcf_tbi_ch, params.ref_genome, low_coverage_bed_ch)
//}





nextflow.enable.dsl=2

// ===== Modules imports =====
include { merge_fastq }            from './modules/merge_fastq.nf'
include { trim_reads }             from './modules/trim_reads.nf'
include { parse_fastq_qc }         from './modules/parse_fastq_qc.nf'
include { mapping_bam }            from './modules/mapping_bam.nf'
include { parse_bam_qc }           from './modules/parse_bam_qc.nf'
include { bam_indexing }           from './modules/bam_indexing.nf'
include { bam_coverage }           from './modules/bam_coverage.nf'
include { low_coverage_filtering } from './modules/low_coverage_filtering.nf'
include { variant_calling }        from './modules/variant_calling.nf'
include { parse_variant_qc }      from './modules/parse_variant_qc.nf'
include { consensus_generation }   from './modules/consensus_generation.nf'

// ===== Param√®tres =====
params.input = null
params.sample = null
params.ref_genome = "/home/etudiant/fatemeh/hbv_pipeline/Ref/sequence.fasta"
params.adapter = "/home/etudiant/fatemeh/hbv_pipeline/primer/HBV_primer.fasta"
params.min_len = 150
params.max_len = 1200
params.min_qual = 10
params.cpu = 10

workflow {

    if (params.input == null) {
        error "‚ùå Vous devez donner un chemin avec --input"
    }

    // D√©tection du type d'entr√©e
    if (file(params.input).isDirectory()) {
        def files = file(params.input).listFiles()

        if (files.any { it.isDirectory() }) {
            println "üîπ Dossier multiplex d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.input}/*/*.fastq", flat: true)
        } else {
            error "‚ùå Aucun dossier multiplex d√©tect√©."
        }
    } else {
        error "‚ùå Le chemin donn√© avec --input n'est pas valide"
    }

    // Pipeline complet par barcode
    merged_fastq_ch = merge_fastq(fastq_ch)

    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        params.adapter,
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    fastq_qc_ch = parse_fastq_qc(merged_fastq_ch)

    mapped_bam_ch = mapping_bam(trimmed_fastq_ch, params.ref_genome)

    bam_qc_ch = parse_bam_qc(mapped_bam_ch)

    bam_index_outputs = bam_indexing(mapped_bam_ch)
    indexed_bam_ch = bam_index_outputs[0]
    indexed_bai_ch = bam_index_outputs[1]

    coverage_bed_ch = bam_coverage(mapped_bam_ch, params.ref_genome)

    filtered_bam_ch = low_coverage_filtering(coverage_bed_ch)

    variant_vcf_ch  = variant_calling(indexed_bam_ch, params.ref_genome)

    variant_qc_ch   = parse_variant_qc(variant_vcf_ch)

    consensus_fa_ch = consensus_generation(indexed_bam_ch, params.ref_genome)

    println "‚úÖ Pipeline termin√© pour tous les barcodes."
}






//////////////////////////////////////////

nextflow.enable.dsl=2

// ===== Modules imports =====
include { merge_fastq }            from './modules/merge_fastq.nf'
//include { trim_reads }             from './modules/trim_reads.nf'
//include { parse_fastq_qc }         from './modules/parse_fastq_qc.nf'
//include { mapping_bam }            from './modules/mapping_bam.nf'
//include { parse_bam_qc }           from './modules/parse_bam_qc.nf'
//include { bam_indexing }           from './modules/bam_indexing.nf'
//include { bam_coverage }           from './modules/bam_coverage.nf'
//include { low_coverage_filtering } from './modules/low_coverage_filtering.nf'
//include { variant_calling }        from './modules/variant_calling.nf'
//include { parse_variant_qc }      from './modules/parse_variant_qc.nf'
//include { consensus_generation }   from './modules/consensus_generation.nf'

// ===== Param√®tres =====
params.input = null
params.sample = null
params.ref_genome = "/home/etudiant/fatemeh/hbv_pipeline/Ref/sequence.fasta"
params.adapter = "/home/etudiant/fatemeh/hbv_pipeline/primer/HBV_primer.fasta"
params.min_len = 150
params.max_len = 1200
params.min_qual = 10
params.cpu = 10

workflow {

    if (params.input == null) {
        error "‚ùå Vous devez donner un chemin avec --input"
    }

    // D√©tection du type d'entr√©e
    if (file(params.input).isFile()) {
        if (params.input.endsWith('.fastq') || params.input.endsWith('.fastq.gz')) {
            println "üîπ Fichier FASTQ d√©tect√©"
            fastq_ch = Channel.of([params.sample ?: "sample1", file(params.input)])
        } else if (params.input.endsWith('.fasta')) {
            println "üîπ Fichier FASTA d√©tect√©"
            fasta_ch = Channel.of([params.sample ?: "sample1", file(params.input)])
        } else if (params.input.endsWith('.csv')) {
            println "üîπ Fichier CSV d√©tect√©"
            csv_ch = Channel.of(file(params.input))
        } else if (params.input.endsWith('.tar.gz')) {
            error "‚ùóÔ∏è Archive d√©tect√©e. Veuillez la d√©compresser avant de lancer le pipeline."
        } else {
            error "‚ùå Format de fichier non reconnu."
        }

    } else if (file(params.input).isDirectory()) {
        def files = file(params.input).listFiles()

        if (files.any { it.isDirectory() }) {
            println "üîπ Dossier multiplex d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.input}/*/*.fastq", flat: true)
            fasta_ch = Channel.fromFilePairs("${params.input}/*/*.fasta", flat: true)
        } else if (files.any { it.name.endsWith('.fastq') || it.name.endsWith('.fastq.gz') }) {
            println "üîπ Dossier plat FASTQ d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.input}/*.{fastq,fastq.gz}", flat: true)
        } else if (files.any { it.name.endsWith('.fasta') }) {
            println "üîπ Dossier plat FASTA d√©tect√©"
            fasta_ch = Channel.fromFilePairs("${params.input}/*.fasta", flat: true)
        } else {
            error "‚ùå Aucun fichier FASTQ, FASTA ou CSV trouv√©."
        }
    } else {
        error "‚ùå Le chemin donn√© avec --input n'est pas valide"
    }

    if (fastq_ch) {
        merged_fastq_ch = merge_fastq(fastq_ch)

        trimmed_fastq_ch = trim_reads(
            merged_fastq_ch,
            params.adapter,
            params.min_len,
            params.max_len,
            params.min_qual,
            params.cpu
        )

        fastq_qc_ch = parse_fastq_qc(merged_fastq_ch)

  //      mapped_bam_ch = mapping_bam(trimmed_fastq_ch, params.ref_genome)

        bam_qc_ch = parse_bam_qc(mapped_bam_ch)

        bam_index_outputs = bam_indexing(mapped_bam_ch)
        indexed_bam_ch = bam_index_outputs[0]
        indexed_bai_ch = bam_index_outputs[1]

        coverage_bed_ch = bam_coverage(mapped_bam_ch, params.ref_genome)

        filtered_bam_ch = low_coverage_filtering(coverage_bed_ch)

        variant_vcf_ch  = variant_calling(indexed_bam_ch, params.ref_genome)

        variant_qc_ch   = parse_variant_qc(variant_vcf_ch)

        consensus_fa_ch = consensus_generation(indexed_bam_ch, params.ref_genome)
    }
}




////////////

//process merge_fastq {
    //input:
        //path in_fastq
      //  val sample_name

    //output:
    //    path "${sample_name}.fastq.gz"

  //  script:
        """
        cat ${in_fastq} > ${sample_name}.fastq.gz
        """
//}





process merge_fastq {
    input:
        val sample_name
        path in_fastq

    output:
        path "${sample_name}.fastq.gz"

    script:
        """
        cat ${in_fastq} | gzip > ${sample_name}.fastq.gz
        """
}


nextflow.enable.dsl=2

// ===== Importation des modules =====
include { merge_fastq }            from './modules/merge_fastq.nf'
include { trim_reads }             from './modules/trim_reads.nf'
include { parse_fastq_qc }         from './modules/parse_fastq_qc.nf'
include { mapping_bam }            from './modules/mapping_bam.nf'
include { parse_bam_qc }           from './modules/parse_bam_qc.nf'
include { bam_indexing }           from './modules/bam_indexing.nf'
include { bam_coverage }           from './modules/bam_coverage.nf'
include { low_coverage_filtering } from './modules/low_coverage_filtering.nf'
include { variant_calling }        from './modules/variant_calling.nf'
include { parse_variant_qc }      from './modules/parse_variant_qc.nf'
include { consensus_generation }   from './modules/consensus_generation.nf'

// ===== Param√®tres =====
params.input = null
params.sample = null
params.ref_genome = "/home/etudiant/fatemeh/hbv_pipeline/Ref/sequence.fasta"
params.adapter = "/home/etudiant/fatemeh/hbv_pipeline/primer/HBV_primer.fasta"
params.min_len = 150
params.max_len = 1200
params.min_qual = 10
params.cpu = 10

// ===== Processus de d√©compression =====
process decompress_archive {
    input:
    path archive_file

    output:
    path "data_extracted"

    script:
    """
    mkdir -p data_extracted
    if [[ "${archive_file}" == *.tar.gz || "${archive_file}" == *.tgz ]]; then
        tar -xzf ${archive_file} -C data_extracted || true
    elif [[ "${archive_file}" == *.tar ]]; then
        tar -xf ${archive_file} -C data_extracted || true
    elif [[ "${archive_file}" == *.zip ]]; then
        unzip -q ${archive_file} -d data_extracted || true
    elif [[ "${archive_file}" == *.gz ]]; then
        gunzip -c ${archive_file} > data_extracted/$(basename ${archive_file} .gz) || true
    fi
    """
}

workflow {

    if (params.input == null) {
        error "‚ùå Veuillez sp√©cifier un chemin d'entr√©e avec --input"
    }

    def input_path = file(params.input)
    def fastq_dir
    Channel fastq_ch

    if (input_path.name.endsWith('.zip') || input_path.name.endsWith('.tar.gz') || input_path.name.endsWith('.tar') || input_path.name.endsWith('.gz')) {
        println "üî∏ Archive d√©tect√©e, tentative de d√©compression..."
        decompress_ch = decompress_archive(input_path)
        fastq_dir = "data_extracted"
    } else {
        fastq_dir = params.input
    }

    def barcode_dirs = file(fastq_dir).listFiles().findAll { it.isDirectory() && it.name.startsWith('barcode') }

    if (file(fastq_dir).isFile()) {
        println "üîπ Mode fichier unique d√©tect√©."
        fastq_ch = Channel.of([fastq_dir, fastq_dir])
    }
    else if (barcode_dirs.size() > 0) {
        println "üîπ Mode multiplex√© d√©tect√©."
        fastq_ch = Channel.fromFilePairs("${fastq_dir}/barcode*/**/*.{fastq,fastq.gz}", flat: true)
    }
    else {
        println "üîπ Mode dossier simple d√©tect√©."
        fastq_ch = Channel.fromFilePairs("${fastq_dir}/**/*.{fastq,fastq.gz}", flat: true)
    }

    fastq_ch.view { it -> println "üìÑ Fichier FASTQ trouv√© : ${it}" }

    merged_fastq_ch = merge_fastq(fastq_ch)

    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        params.adapter,
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    fastq_qc_ch = parse_fastq_qc(merged_fastq_ch)

    mapped_bam_ch = mapping_bam(trimmed_fastq_ch, params.ref_genome)

    bam_qc_ch = parse_bam_qc(mapped_bam_ch)

    bam_index_outputs = bam_indexing(mapped_bam_ch)
    indexed_bam_ch = bam_index_outputs[0]
    indexed_bai_ch = bam_index_outputs[1]

    coverage_bed_ch = bam_coverage(mapped_bam_ch, params.ref_genome)

    filtered_bam_ch = low_coverage_filtering(coverage_bed_ch)

    variant_vcf_ch  = variant_calling(indexed_bam_ch, params.ref_genome)

    variant_qc_ch   = parse_variant_qc(variant_vcf_ch)

    consensus_fa_ch = consensus_generation(indexed_bam_ch, params.ref_genome)

    println "‚úÖ Pipeline termin√© pour les fichiers FASTQ."
}




//process merge_fastq {

  //  input:
   // tuple val(sample_id), path(reads)

  //  output:
  //  tuple val(sample_id), path("${sample_id}.fastq.gz")

   // script:
    """
   // cat ${reads.join(' ')} | gzip > ${sample_id}.fastq.gz
    """
//}




nextflow.enable.dsl=2

// ===== Modules imports =====
include { decompress_archive } from './modules/decompress_archive.nf'
include { merge_fastq }        from './modules/merge_fastq.nf'
include { trim_reads }         from './modules/trim_reads.nf'
include { parse_fastq_qc }     from './modules/parse_fastq_qc.nf'

// ===== Param√®tres =====
params.input = null
params.adapter = "/home/etudiant/fatemeh/hbv_pipeline/primer/HBV_primer.fasta"
params.min_len = 150
params.max_len = 1200
params.min_qual = 10
params.cpu = 10

workflow {

    if (params.input == null) {
        error "‚ùå Vous devez donner un chemin avec --input"
    }

    def fastq_ch

    // ==== D√©compression automatique si archive ====
    if (file(params.input).isFile() && (params.input.endsWith(".tar.gz") || params.input.endsWith(".zip") || params.input.endsWith(".gz"))) {
        println "üü¢ Archive d√©tect√©e : d√©compression automatique"

        def archive_ch = Channel.of(file(params.input))
        def decompressed_ch = decompress_archive(archive_ch)

        fastq_ch = decompressed_ch.flatMap { folder ->
            println "üîπ Dossier multiplex d√©tect√© apr√®s d√©compression"
            Channel.fromPath("${folder}/*/*.fastq.gz")
                  .map { file -> tuple(file.parent.name, file) }
                  .groupTuple()
        }
    }

    // ==== Dossier multiplex ====
    else if (file(params.input).isDirectory()) {
        println "üîπ Dossier multiplex d√©tect√©"
        fastq_ch = Channel.fromPath("${params.input}/*/*.fastq.gz")
                          .map { file -> tuple(file.parent.name, file) }
                          .groupTuple()
    }
    else {
        error "‚ùå Le chemin donn√© n'est pas valide"
    }

    // ===== Pipeline =====
    def merged_fastq_ch = merge_fastq(fastq_ch)

    def trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        file(params.adapter),
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    parse_fastq_qc(trimmed_fastq_ch)
}


process parse_fastq_qc {

    input:
    tuple val(sample_id), path(in_fastq)

    output:
    path "${sample_id}_qc.txt"

    script:
    """
    nb_lines=\$(zcat ${in_fastq} | wc -l)
    file_size=\$(du -h ${in_fastq} | cut -f1)

    {
        echo "Sample ID       : ${sample_id}"
        echo "Nombre de lignes: \$nb_lines"
        echo "Taille fichier  : \$file_size"
    } > ${sample_id}_qc.txt
    """
}



process trim_reads {

    input:
    tuple val(sample_id), path(in_fastq)
    path in_adapter
    val min_len
    val max_len
    val min_qual
    val cpu

    output:
    tuple val(sample_id), path("${sample_id}_trim_reads.fastq.gz")

    script:
    """
    cutadapt -j ${cpu} \\
        -a file:${in_adapter} \\
        --discard-untrimmed \\
        -m ${min_len} \\
        -M ${max_len} \\
        -q ${min_qual} \\
        -o ${sample_id}_trim_reads.fastq.gz \\
        ${in_fastq}
    """
}



process merge_fastq {

    input:
    tuple val(sample_id), path(reads)

    output:
    tuple val(sample_id), path("${sample_id}.fastq")

    script:
    """
    cat ${reads.join(' ')} > ${sample_id}.fastq
    """
}

/////////////////////////////////////////////////////////******************************************************

nextflow.enable.dsl=2

// ===== Modules imports =====
include { merge_fastq }        from './modules/merge_fastq.nf'
include { trim_reads }         from './modules/trim_reads.nf'
include { parse_fastq_qc }     from './modules/parse_fastq_qc.nf'
include { decompress_archive } from './modules/decompress_archive.nf'

// ===== Param√®tres =====
params.input = null
params.sample = null
params.adapter = "/home/etudiant/fatemeh/hbv_pipeline/primer/HBV_primer.fasta"
params.min_len = 150
params.max_len = 1200
params.min_qual = 10
params.cpu = 10

workflow {

    if (params.input == null) {
        error "‚ùå Vous devez donner un chemin avec --input"
    }

    def fastq_ch

    // ==== Si l'entr√©e est un fichier compress√© ====
    if (file(params.input).isFile() && (params.input.endsWith(".tar.gz") || params.input.endsWith(".zip") || params.input.endsWith(".gz"))) {
        println "üü¢ Archive d√©tect√©e : d√©compression automatique"

        archive_ch = Channel.of(file(params.input))
        decompressed_ch = decompress_archive(archive_ch)

        decompressed_ch.subscribe { folder ->
            def files = file(folder).listFiles()
            if (files.any { it.isDirectory() }) {
                println "üîπ Dossier multiplex d√©tect√© apr√®s d√©compression"
                fastq_ch = Channel.fromFilePairs("${folder}/*/*.fastq.gz", flat: true)
            } else {
                println "üîπ Dossier plat d√©tect√© apr√®s d√©compression"
                fastq_ch = Channel.fromFilePairs("${folder}/*.fastq.gz", flat: true)
            }
        }
    }

    // ==== Si l'entr√©e est un dossier ====
    else if (file(params.input).isDirectory()) {
        def files = file(params.input).listFiles()
        if (files.any { it.isDirectory() }) {
            println "üîπ Dossier multiplex d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.input}/*/*.fastq.gz", flat: true)
        } else {
            println "üîπ Dossier plat d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.input}/*.fastq.gz", flat: true)
        }
    }

    // ==== Si l'entr√©e est un fichier FASTQ ====
    else if (file(params.input).isFile()) {
        println "üîπ Fichier unique d√©tect√©"
        def sample_name = params.sample ?: file(params.input).baseName
        fastq_ch = Channel.of([sample_name, file(params.input)])
    }

    else {
        error "‚ùå Le chemin donn√© n'est pas valide"
    }

    // ===== Pipeline =====
    merged_fastq_ch = merge_fastq(fastq_ch)

    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        file(params.adapter),
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    parse_fastq_qc(trimmed_fastq_ch)
}










nextflow.enable.dsl=2

// ===== Modules imports =====
include { merge_fastq }        from './modules/merge_fastq.nf'
include { trim_reads }         from './modules/trim_reads.nf'
include { parse_fastq_qc }     from './modules/parse_fastq_qc.nf'
include { decompress_archive } from './modules/decompress_archive.nf'

// ===== Param√®tres =====
params.input = null
params.sample = null
params.adapter = "/home/etudiant/fatemeh/hbv_pipeline/primer/HBV_primer.fasta"
params.min_len = 150
params.max_len = 1200
params.min_qual = 10
params.cpu = 10

workflow {

    if (params.input == null) {
        error "‚ùå Vous devez donner un chemin avec --input"
    }

    def fastq_ch

    // ==== Si l'entr√©e est un fichier compress√© ====
    if (file(params.input).isFile() && (params.input.endsWith(".tar.gz") || params.input.endsWith(".zip") || params.input.endsWith(".gz"))) {
        println "üü¢ Archive d√©tect√©e : d√©compression automatique"

        archive_ch = Channel.of(file(params.input))
        decompressed_ch = decompress_archive(archive_ch)

        decompressed_ch.subscribe { folder ->
            def files = file(folder).listFiles()
            if (files.any { it.isDirectory() }) {
                println "üîπ Dossier multiplex d√©tect√© apr√®s d√©compression"
                fastq_ch = Channel.fromFilePairs("${folder}/*/*.fastq.gz", flat: true)
            } else {
                println "üîπ Dossier plat d√©tect√© apr√®s d√©compression"
                fastq_ch = Channel.fromFilePairs("${folder}/*.fastq.gz", flat: true)
            }
        }
    }

    // ==== Si l'entr√©e est un dossier ====
    else if (file(params.input).isDirectory()) {
        def files = file(params.input).listFiles()
        if (files.any { it.isDirectory() }) {
            println "üîπ Dossier multiplex d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.input}/*/*.fastq.gz", flat: true)
        } else {
            println "üîπ Dossier plat d√©tect√©"
            fastq_ch = Channel.fromFilePairs("${params.input}/*.fastq.gz", flat: true)
        }
    }

    // ==== Si l'entr√©e est un fichier FASTQ ====
    else if (file(params.input).isFile()) {
        println "üîπ Fichier unique d√©tect√©"
        def sample_name = params.sample ?: file(params.input).baseName
        fastq_ch = Channel.of([sample_name, file(params.input)])
    }

    else {
        error "‚ùå Le chemin donn√© n'est pas valide"
    }

    // ===== Pipeline =====
    merged_fastq_ch = merge_fastq(fastq_ch)

    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        file(params.adapter),
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    parse_fastq_qc(trimmed_fastq_ch)
}


   merged_fastq_ch = merge_fastq(input_ch)
    trimmed_fastq_ch = trim_reads(merged_fastq_ch, file(params.adapter), params.min_len, params.max_len, params.min_qual, params.cpu)
    parse_fastq_qc(trimmed_fastq_ch)
    mapping_bam(trimmed_fastq_ch, file(params.ref_genome), params.cpu)





    nextflow.enable.dsl=2

// ===== Modules imports =====
include { decompress_archive } from './modules/decompress_archive.nf'
include { merge_fastq }        from './modules/merge_fastq.nf'
include { trim_reads }         from './modules/trim_reads.nf'
include { parse_fastq_qc }     from './modules/parse_fastq_qc.nf'
include { mapping_bam }        from './modules/mapping_bam.nf'

workflow {

    if (params.input == null) {
        error "‚ùå Vous devez donner un chemin avec --input"
    }

    def input_ch

    // ==== D√©tection automatique ====
    if (file(params.input).isFile()) {
        println "üîπ Fichier unique d√©tect√©"
        def sample_name = params.sample ?: file(params.input).baseName
        input_ch = Channel.of([sample_name, file(params.input)])
    }

    else if (file(params.input).isDirectory()) {
        def files = file(params.input).listFiles()

        if (files.any { it.isDirectory() }) {
            println "üîπ Dossier multiplex d√©tect√©"
            input_ch = Channel
                .fromPath("${params.input}/*/*.{fastq.gz,bam}")
                .map { tuple(it.parent.name, it) }
                .groupTuple()
        } else {
            println "üîπ Dossier plat d√©tect√©"
            input_ch = Channel.fromFilePairs("${params.input}/*.{fastq.gz,bam}", flat: true)
        }
    }

    else if (file(params.input).isFile() && (params.input.endsWith(".tar.gz") || params.input.endsWith(".zip") || params.input.endsWith(".gz"))) {
        println "üü¢ Archive d√©tect√©e : d√©compression automatique"

        def archive_ch = Channel.of(file(params.input))
        def decompressed_ch = decompress_archive(archive_ch)

        decompressed_ch.subscribe { folder ->
            def files = file(folder).listFiles()
            if (files.any { it.isDirectory() }) {
                println "üîπ Dossier multiplex d√©tect√© apr√®s d√©compression"
                input_ch = Channel
                    .fromPath("${folder}/*/*.{fastq.gz,bam}")
                    .map { tuple(it.parent.name, it) }
                    .groupTuple()
            } else {
                println "üîπ Dossier plat d√©tect√© apr√®s d√©compression"
                input_ch = Channel.fromFilePairs("${folder}/*.{fastq.gz,bam}", flat: true)
            }
        }
    }

    else {
        error "‚ùå Le chemin donn√© n'est pas valide"
    }

    // ===== Pipeline =====

    merged_fastq_ch = merge_fastq(input_ch)
    trimmed_fastq_ch = trim_reads(merged_fastq_ch, file(params.adapter), params.min_len, params.max_len, params.min_qual, params.cpu)
    parse_fastq_qc(trimmed_fastq_ch)
    mapping_bam(trimmed_fastq_ch, file(params.ref_genome), params.cpu)
}


//nextflow.enable.dsl=2

// ===== Modules imports =====
include { merge_fastq }        from './modules/merge_fastq.nf'
include { trim_reads }         from './modules/trim_reads.nf'
include { parse_fastq_qc }     from './modules/parse_fastq_qc.nf'
include { decompress_archive } from './modules/decompress_archive.nf'

// ===== Param√®tres =====
params.input        = null
params.sample       = null
params.adapter      = "./primer/HBV_primer.fasta"
params.ref_genome   = "./Ref/sequence.fasta"
params.result_dir   = "./results"
params.min_len      = 150
params.max_len      = 1200
params.min_qual     = 10
params.cpu          = 4

// ===== Fonctions =====
/**
 * Fonction qui d√©tecte le type d'entr√©e
 * et retourne un Channel dynamique
 */
def detect_input(input_path) {

    def input_file = file(input_path)

    if (!input_file.exists()) {
        error "‚ùå Chemin invalide : ${input_path}"
    }

    // ==== Cas archive ====
    if (input_file.isFile() && (input_file.name.endsWith(".tar.gz") || input_file.name.endsWith(".zip") || input_file.name.endsWith(".gz"))) {
        println "üü¢ Archive d√©tect√©e : d√©compression automatique"

        def archive_ch = Channel.of(input_file)
        def decompressed_ch = decompress_archive(archive_ch)

        return decompressed_ch.map { folder ->
            def files = file(folder).listFiles()
            if (files.any { it.isDirectory() }) {
                println "üîπ Dossier multiplex d√©tect√© apr√®s d√©compression"
                return Channel.fromFilePairs("${folder}/*/*.fastq.gz", flat: true)
            } else {
                println "üîπ Dossier plat d√©tect√© apr√®s d√©compression"
                return Channel.fromFilePairs("${folder}/*.fastq.gz", flat: true)
            }
        }.flatten()
    }

    // ==== Cas dossier ====
    if (input_file.isDirectory()) {
        def files = input_file.listFiles()
        if (files.any { it.isDirectory() }) {
            println "üîπ Dossier multiplex d√©tect√©"
            return Channel.fromFilePairs("${input_path}/*/*.fastq.gz", flat: true)
        } else {
            println "üîπ Dossier plat d√©tect√©"
            return Channel.fromFilePairs("${input_path}/*.fastq.gz", flat: true)
        }
    }

    // ==== Cas fichier FASTQ ====
    if (input_file.isFile()) {
        println "üîπ Fichier unique d√©tect√©"
        def sample_name = params.sample ?: input_file.baseName
        return Channel.of([sample_name, input_file])
    }

    error "‚ùå Format non reconnu"
}

// ===== Pipeline =====
workflow {

    if (params.input == null) {
        error "‚ùå Vous devez fournir --input"
    }

    fastq_ch = detect_input(params.input)

    merged_fastq_ch = merge_fastq(fastq_ch)

    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        file(params.adapter),
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    parse_fastq_qc(trimmed_fastq_ch)
}






//////////////////******

nextflow.enable.dsl=2

// ===== Modules imports =====
include { merge_fastq }        from './modules/merge_fastq.nf'
include { trim_reads }         from './modules/trim_reads.nf'
include { parse_fastq_qc }     from './modules/parse_fastq_qc.nf'
include { decompress_archive } from './modules/decompress_archive.nf'

// ===== Param√®tres =====
params.input        = null
params.sample       = null
params.adapter      = "./primer/HBV_primer.fasta"
params.ref_genome   = "./Ref/sequence.fasta"
params.result_dir   = "./results"
params.min_len      = 150
params.max_len      = 1200
params.min_qual     = 10
params.cpu          = 4

// ===== Pipeline =====
workflow {

    if (params.input == null) {
        error "‚ùå Vous devez fournir --input"
    }

    def input_file = file(params.input)

    // ==== Dossier multiplex ====
    if (input_file.isDirectory() && input_file.list().any { file("${params.input}/${it}").isDirectory() }) {
        println "üîπ Dossier multiplex d√©tect√©"

        Channel
            .fromPath("${params.input}/*/*.fastq.gz")
            .collect()
            .map { files ->
                files.groupBy { it.parent.name }
                    .collect { key, list -> tuple(key, list.sort()) }
            }
            .flatten()
            .set { fastq_ch }
    }

    // ==== Dossier plat ====
    else if (input_file.isDirectory()) {
        println "üîπ Dossier plat d√©tect√©"

        Channel
            .fromPath("${params.input}/*.fastq.gz")
            .collect()
            .map { files -> tuple("sample", files.sort()) }
            .set { fastq_ch }
    }

    // ==== Fichier unique ====
    else if (input_file.isFile()) {
        println "üîπ Fichier unique d√©tect√©"

        def sample_name = params.sample ?: input_file.baseName
        Channel
            .of(tuple(sample_name, [input_file]))
            .set { fastq_ch }
    }

    else {
        error "‚ùå Chemin invalide : ${params.input}"
    }

    // ===== Pipeline execution =====
    merged_fastq_ch = merge_fastq(fastq_ch)

    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        file(params.adapter),
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    parse_fastq_qc(trimmed_fastq_ch)
}


///////////////


nextflow.enable.dsl=2

// ===== Modules imports =====
include { decompress_archive } from './modules/decompress_archive.nf'
include { merge_fastq }        from './modules/merge_fastq.nf'
include { trim_reads }         from './modules/trim_reads.nf'
include { parse_fastq_qc }     from './modules/parse_fastq_qc.nf'


// ===== Param√®tres =====
params.input        = null
params.sample       = null
params.adapter      = "./primer/HBV_primer.fasta"
params.ref_genome   = "./Ref/sequence.fasta"
params.result_dir   = "./results"
params.min_len      = 150
params.max_len      = 1200
params.min_qual     = 10
params.cpu          = 4

// ===== Pipeline =====
workflow {

    if (params.input == null) {
        error "‚ùå Vous devez fournir --input"
    }

    def input_file = file(params.input)

    // ==== Dossier multiplex ====
    if (input_file.isDirectory() && input_file.list().any { file("${params.input}/${it}").isDirectory() }) {
        println "üîπ Dossier multiplex d√©tect√©"

        Channel
            .fromPath("${params.input}/*/*.fastq.gz")
            .collect()
            .filter { it.exists() && it.size() > 0 }  // Ignore empty or missing files
            .map { files ->
                files.groupBy { it.parent.name }
                    .collect { key, list -> tuple(key, list.sort()) }
            }
            .filter { it[1].size() > 0 }  // Ensure that barcode has valid files
            .flatten()
            .set { fastq_ch }
    }

    // ==== Dossier plat ====
    else if (input_file.isDirectory()) {
        println "üîπ Dossier plat d√©tect√©"

        Channel
            .fromPath("${params.input}/*.fastq.gz")
            .collect()
            .filter { it.exists() && it.size() > 0 }  // Ignore empty or missing files
            .map { files -> tuple("sample", files.sort()) }
            .set { fastq_ch }
    }

    // ==== Fichier unique ====
    else if (input_file.isFile()) {
        println "üîπ Fichier unique d√©tect√©"

        def sample_name = params.sample ?: input_file.baseName
        Channel
            .of(tuple(sample_name, [input_file]))
            .set { fastq_ch }
    }

    else {
        error "‚ùå Chemin invalide : ${params.input}"
    }

    // ===== Pipeline execution =====
    merged_fastq_ch = merge_fastq(fastq_ch)

    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        file(params.adapter),
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    parse_fastq_qc(trimmed_fastq_ch)
}



nextflow.enable.dsl=2

// ========== MODULES ==========
include { decompress_archive } from './modules/decompress_archive.nf'
include { merge_fastq }        from './modules/merge_fastq.nf'
include { trim_reads }         from './modules/trim_reads.nf'
include { parse_fastq_qc }     from './modules/parse_fastq_qc.nf'

// ========== PARAM√àTRES ==========
params.input        = null
params.sample       = null
params.adapter      = "./primer/HBV_primer.fasta"
params.ref_genome   = "./Ref/sequence.fasta"
params.result_dir   = "./results"
params.min_len      = 150
params.max_len      = 1200
params.min_qual     = 10
params.cpu          = 4

// ========== WORKFLOW PRINCIPAL ==========
workflow {

    // ‚úÖ V√©rifie que --input est fourni
    if (params.input == null) {
        error "‚ùå Vous devez fournir --input"
    }

    def input_file = file(params.input)

    // ======== CAS 1 : Archive compress√©e (.tar.gz, .zip, .gz) ========
    if (input_file.name.endsWith(".tar.gz") || input_file.name.endsWith(".zip") || input_file.name.endsWith(".gz")) {
        println "üîπ Archive compress√©e d√©tect√©e"

        Channel
            .of(input_file)
            .set { archive_ch }

        decompress_archive(archive_ch)
            .map { file("decompressed") }
            .set { real_input_dir }

        // D√©tection plat ou multiplex apr√®s d√©compression
        real_input_dir
            .map { folder ->
                def is_multiplex = folder.list().any { file("${folder}/${it}").isDirectory() }

                if (is_multiplex) {
                    println "üîπ Dossier multiplex d√©tect√©"
                    def all_files = file("${folder}/*/*.fastq.gz").collect()
                    def grouped = all_files.groupBy { it.parent.name }
                    return grouped.collect { name, list -> tuple(name, list.sort()) }
                } else {
                    println "üîπ Dossier plat d√©tect√©"
                    def files = file("${folder}/*.fastq.gz").collect()
                    return [tuple("sample", files.sort())]
                }
            }
            .flatten()
            .set { fastq_ch }
    }

    // ======== CAS 2 : Dossier plat (fichiers uniquement) ========
    else if (
        input_file.isDirectory() &&
        !input_file.list().any { file("${params.input}/${it}").isDirectory() }
    ) {
        println "üîπ Dossier plat d√©tect√©"

        def files = file("${params.input}/*.fastq.gz").collect()
        Channel
            .of(tuple("sample", files.sort()))
            .set { fastq_ch }
    }

    // ======== CAS 3 : Dossier multiplex (avec sous-dossiers) ========
    else if (
        input_file.isDirectory() &&
        input_file.list().any { file("${params.input}/${it}").isDirectory() }
    ) {
        println "üîπ Dossier multiplex d√©tect√©"

        def all_files = file("${params.input}/*/*.fastq.gz").collect()
        def grouped = all_files.groupBy { it.parent.name }
        def all_tuples = grouped.collect { name, list -> tuple(name, list.sort()) }

        Channel
            .from(all_tuples)
            .set { fastq_ch }
    }

    // ======== CAS 4 : Fichier FASTQ unique ========
    else if (input_file.name.endsWith(".fastq.gz")) {
        println "üîπ Fichier FASTQ unique d√©tect√©"

        def sample_name = params.sample ?: input_file.baseName

        Channel
            .of(tuple(sample_name, [input_file]))
            .set { fastq_ch }
    }

    // ======== ERREUR : Chemin invalide ========
    else {
        error "‚ùå Chemin invalide ou format non support√© : ${params.input}"
    }

    // ========== PIPELINE BIOINFORMATIQUE ==========

    merged_fastq_ch = merge_fastq(fastq_ch)

    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        file(params.adapter),
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    parse_fastq_qc(trimmed_fastq_ch)
}


////////////////////*

nextflow.enable.dsl=2

// ========== MODULES ==========
include { decompress_archive } from './modules/decompress_archive.nf'
include { merge_fastq }        from './modules/merge_fastq.nf'
include { trim_reads }         from './modules/trim_reads.nf'
include { parse_fastq_qc }     from './modules/parse_fastq_qc.nf'

// ========== PARAM√àTRES ==========
params.input        = null
params.sample       = null
params.adapter      = "./primer/HBV_primer.fasta"
params.result_dir   = "./results"
params.min_len      = 150
params.max_len      = 1200
params.min_qual     = 10
params.cpu          = 4

// ========== WORKFLOW PRINCIPAL ==========
workflow {

    // V√©rification que --input est fourni
    if (params.input == null) {
        error "‚ùå Vous devez fournir --input"
    }

    def input_file = file(params.input)

    // ===========================
    // üì¶ CAS 1 : Archive compress√©e (tar.gz, zip, gz)
    // ===========================
    if (input_file.name.endsWith(".tar.gz") || input_file.name.endsWith(".zip") || input_file.name.endsWith(".gz")) {
        println "üîπ Archive compress√©e d√©tect√©e"

        Channel
            .of(input_file)
            .set { archive_ch }

        decompress_archive(archive_ch)
            .map { file("decompressed") }
            .set { real_input_dir }
    }

    // ===========================
    // üìÅ CAS 2 : Dossier d√©tect√©
    // ===========================
    else if (input_file.isDirectory()) {
        println "üîπ Dossier d√©tect√©"
        Channel
            .of(input_file)
            .set { real_input_dir }
    }

    // ===========================
    // üìÑ CAS 3 : Fichier FASTQ unique
    // ===========================
    else if (input_file.name.endsWith(".fastq.gz")) {
        println "üîπ Fichier FASTQ unique d√©tect√©"

        def sample_name = params.sample ?: input_file.baseName
        Channel
            .of(tuple(sample_name, [input_file]))
            .set { fastq_ch }

        // Ex√©cution du pipeline directement pour le fichier unique
        merged_fastq_ch = merge_fastq(fastq_ch)

        trimmed_fastq_ch = trim_reads(
            merged_fastq_ch,
            file(params.adapter),
            params.min_len,
            params.max_len,
            params.min_qual,
            params.cpu
        )

        parse_fastq_qc(trimmed_fastq_ch)

        return
    }

    else {
        error "‚ùå Chemin invalide : ${params.input}"
    }

    // ===========================
    // üîç D√âTECTION DANS LE DOSSIER (plat ou multiplex)
    // ===========================
    real_input_dir
        .map { folder ->
            def is_multiplex = folder.list().any { file("${folder}/${it}").isDirectory() }

            if (is_multiplex) {
                println "üîπ Dossier multiplex d√©tect√©"

                def all_files = file("${folder}/*/*.fastq.gz").collect()
                def grouped = all_files.groupBy { it.parent.name }
                return grouped.collect { name, list -> tuple(name, list.sort()) }
            } else {
                println "üîπ Dossier plat d√©tect√©"

                def files = file("${folder}/*.fastq.gz").collect()
                return [tuple("sample", files.sort())]
            }
        }
        .flatten()
        .set { fastq_ch }

    // ===========================
    // üöÄ PIPELINE BIOINFORMATIQUE
    // ===========================
    merged_fastq_ch = merge_fastq(fastq_ch)

    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        file(params.adapter),
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    parse_fastq_qc(trimmed_fastq_ch)
}




//////////////////////////////**/

process merge_fastq {
    input:
    tuple val(sample_id), path(fastq_files)

    output:
    tuple val(sample_id), path("${sample_id}.merged.fastq.gz")

    publishDir "${params.result_dir}/${sample_id}", mode: 'copy'

    script:
    """
    cat ${fastq_files.join(' ')} > ${sample_id}.merged.fastq.gz
    """
}



//***

process trim_reads {
    input:
    tuple val(sample_id), path(fastq_file)
    path adapter_fasta
    val min_len
    val max_len
    val min_qual
    val cpu

    output:
    tuple val(sample_id), path("${sample_id}_trim_reads.fastq.gz")

     publishDir "${params.result_dir}/${sample_id}", mode: 'copy'


    script:
    """
    cutadapt \
        -a file:$adapter_fasta \
        -m $min_len -M $max_len -q $min_qual \
        -j $cpu \
        -o ${sample_id}_trim_reads.fastq.gz \
        $fastq_file
    """
}



/////////

process parse_fastq_qc {
    input:
    tuple val(sample_id), path(trimmed_fastq)

    output:
    path("${sample_id}_qc.txt")

    publishDir "${params.result_dir}/${sample_id}", mode: 'copy'

    script:
    """
    echo "QC report for $sample_id" > ${sample_id}_qc.txt
    echo "Number of reads: \$(zcat $trimmed_fastq | wc -l | awk '{print \$1/4}')" >> ${sample_id}_qc.txt
    echo "Total lines: \$(zcat $trimmed_fastq | wc -l)" >> ${sample_id}_qc.txt
    """
}



/////////////

process mapping_bam {
    input:
    tuple val(sample_id), path(trimmed_fastq)
    val ref_genome
    val cpu

    output:
    path "${sample_id}.sorted.bam"

    publishDir "${params.result_dir}/${sample_id}", mode: 'copy'

    script:
    """
    set -e
    minimap2 -ax map-ont ${ref_genome} ${trimmed_fastq} | \\
    samtools view -Sb - | \\
    samtools sort -o ${sample_id}.sorted.bam
    """
}





///////////////

process bam_indexing {
    input:
        path sorted_bam  // Fichier BAM tri√©

    output:
        path "${sorted_bam}"  // BAM tri√© index√©
        path "${sorted_bam}.bai"  // Fichier index BAM

    script:
        """
        set -e
        samtools index ${sorted_bam}
        """
}


////////

process bam_coverage {
    input:
        path sorted_bam   // Fichier BAM tri√©
        path ref_genome   // Fichier de r√©f√©rence du g√©nome

    output:
        path "${sorted_bam.baseName}.coverage.bed"  // Fichier .bed contenant la couverture

    script:
        """
        set -e  # Arr√™ter l'ex√©cution en cas d'erreur

        # Calcul de la couverture avec bedtools
        bedtools genomecov -bg -ibam ${sorted_bam} > ${sorted_bam.baseName}.coverage.bed
        """
}


////////////

process low_coverage_filtering {
    input:
        path coverage_bed  // Fichier de couverture g√©n√©r√© par `bam_coverage`

    output:
        path "${coverage_bed.baseName}_low_coverage.bed"  // Fichier BED des r√©gions √† faible couverture

    script:
        """
        set -e  # Arr√™ter l'ex√©cution en cas d'erreur

        # Filtrer les r√©gions avec une couverture < 20
        awk '\$4 < 20 {print \$1, \$2, \$3}' ${coverage_bed} > ${coverage_bed.baseName}_low_coverage.bed
        """
}


/////////////

process variant_calling {
    input:
        path sorted_bam   // Fichier BAM tri√©
        path sorted_bai   // Fichier index BAM
        path ref_genome   // G√©nome de r√©f√©rence

    output:
        path "${sorted_bam.baseName}.vcf.gz"      // Fichier VCF compress√©
        path "${sorted_bam.baseName}.vcf.gz.tbi"  // Index du fichier VCF compress√©

    script:
        """
        set -e  # Arr√™ter l'ex√©cution en cas d'erreur

        # G√©n√©ration du fichier VCF brut
        bcftools mpileup -d 8000 -f ${ref_genome} ${sorted_bam} | \\
        bcftools call -mv | \\
        bcftools filter -e 'QUAL<20' -Ov -o ${sorted_bam.baseName}.vcf

        # Compression et indexation
        bgzip -c ${sorted_bam.baseName}.vcf > ${sorted_bam.baseName}.vcf.gz
        tabix -p vcf ${sorted_bam.baseName}.vcf.gz
        """
}




process consensus_generation {
    input:
        path vcf_gz        // Fichier VCF compress√© contenant les variants
        path vcf_tbi       // Index du fichier VCF compress√©
        path ref_genome    // G√©nome de r√©f√©rence (FASTA)
        path low_coverage  // Fichier BED des r√©gions √† faible couverture

    output:
        path "${vcf_gz.baseName}_consensus.fasta"  // Fichier FASTA final

    script:
        """
        set -e  # Arr√™ter l'ex√©cution en cas d'erreur

        # V√©rification des fichiers
        ls -lh ${vcf_gz} ${vcf_tbi} ${ref_genome} ${low_coverage}

        # G√©n√©ration du consensus
        bcftools consensus -f ${ref_genome} -m ${low_coverage} ${vcf_gz} > ${vcf_gz.baseName}_consensus.fasta
        """
}






// ========== WORKFLOW PRINCIPAL ==========
workflow {

    if (!params.input) {
        error "‚ùå Vous devez fournir un chemin d‚Äôentr√©e avec --input"
    }

    def input_path = file(params.input)
    Channel fastq_ch

    // === CAS 1 : Archive compress√©e ===
    if (input_path.name.endsWith('.tar.gz') || input_path.name.endsWith('.zip') || input_path.name.endsWith('.gz')) {
        decompress_archive(Channel.of(input_path))
            .map { file("decompressed") }
            .map { folder ->
                def subdirs = folder.list().findAll { file("${folder}/${it}").isDirectory() }
                if (subdirs) {
                    println "üîπ Dossier multiplex d√©tect√© apr√®s d√©compression"
                    def grouped = file("${folder}/*/*.fastq.gz").groupBy { it.parent.name }
                    grouped.collect { name, list -> tuple(name, list.sort()) }
                } else {
                    println "üîπ Dossier plat d√©tect√© apr√®s d√©compression"
                    [ tuple("sample", file("${folder}/*.fastq.gz").sort()) ]
                }
            }
            .flatten()
            .set { fastq_ch }
    }

    // === CAS 2 : Dossier plat ===
    else if (input_path.isDirectory() && !input_path.list().any { file("${params.input}/${it}").isDirectory() }) {
        println "üîπ Dossier plat d√©tect√©"
        def files = file("${params.input}/*.fastq.gz").sort()
        fastq_ch = Channel.of(tuple("sample", files))
    }

    // === CAS 3 : Dossier multiplex ===
    else if (input_path.isDirectory() && input_path.list().any { file("${params.input}/${it}").isDirectory() }) {
        println "üîπ Dossier multiplex d√©tect√©"
        def grouped = file("${params.input}/*/*.fastq.gz").groupBy { it.parent.name }
        def tuples = grouped.collect { name, list -> tuple(name, list.sort()) }
        fastq_ch = Channel.from(tuples)
    }

    // === CAS 4 : Fichier unique .fastq.gz ===
    else if (input_path.name.endsWith('.fastq.gz')) {
        println "üîπ Fichier unique FASTQ d√©tect√©"
        def sample_name = params.sample ?: input_path.baseName
        fastq_ch = Channel.of(tuple(sample_name, [input_path]))
    }

    // === Autre cas : erreur
    else {
        error "‚ùå Chemin d‚Äôentr√©e invalide ou non support√© : ${params.input}"
    }

    // === PIPELINE ===

    merged_fastq_ch = merge_fastq(fastq_ch)

    trimmed_fastq_ch = trim_reads(
        merged_fastq_ch,
        file(params.adapter),
        params.min_len,
        params.max_len,
        params.min_qual,
        params.cpu
    )

    parse_fastq_qc(trimmed_fastq_ch)

    mapped_bam_ch = mapping_bam(trimmed_fastq_ch, file(params.ref_genome), params.cpu)

    parse_read_align(mapped_bam_ch)

    indexed_bam_ch = bam_indexing(mapped_bam_ch)

    coverage_ch = bam_coverage(mapped_bam_ch.map { id, bam -> bam }, file(params.ref_genome))

   // lowcov_ch = low_coverage_filtering(coverage_ch)

    //variant_ch = variant_calling(
        mapped_bam_ch.map { id, bam -> bam },
        indexed_bam_ch.map { id, bai -> bai },
        file(params.ref_genome)
    //)

    //parse_variant_qc(variant_ch)

    //consensus_generation(variant_ch, file(params.ref_genome), lowcov_ch)
}



/////////************************///////// 07/04/2025

process low_coverage_filtering {

    input:
    tuple val(sample_id), path(coverage_file)

    output:
    path("${sample_id}_low_coverage.bed")

    publishDir "${params.result_dir ?: './results'}/${sample_id}/low_coverage", mode: 'copy'

    script:
    """
    set -e
    awk '\$4 < 20 {print \$1, \$2, \$3}' ${coverage_file} > ${sample_id}_low_coverage.bed
    """
}

process variant_calling {
    input:
        path sorted_bam
        path sorted_bai
        path ref_genome

    output:
        path "${sorted_bam.baseName}.vcf.gz"
        path "${sorted_bam.baseName}.vcf.gz.tbi"

    publishDir "${params.result_dir ?: './results'}/${sorted_bam.simpleName}", mode: 'copy'

    script:
    """
    bcftools mpileup -d 8000 -f ${ref_genome} ${sorted_bam} | \\
    bcftools call -mv | \\
    bcftools filter -e 'QUAL<20' -Ov -o ${sorted_bam.baseName}.vcf

    bgzip -c ${sorted_bam.baseName}.vcf > ${sorted_bam.baseName}.vcf.gz
    tabix -p vcf ${sorted_bam.baseName}.vcf.gz
    """
}




process variant_calling {

    input:
    tuple val(sample_id), path(bam_file), path(bai_file)
    path ref_genome

    output:
    path("${sample_id}.sorted.vcf.gz")
    path("${sample_id}.sorted.vcf.gz.tbi")

    publishDir "${params.result_dir ?: './results'}/${sample_id}", mode: 'copy'

    script:
    """
    set -e

    # Appel de variants avec bcftools
    bcftools mpileup -Ou -f ${ref_genome} ${bam_file} | \\
    bcftools call -mv -Oz -o ${sample_id}.sorted.vcf.gz

    # Indexation du VCF compress√©
    bcftools index ${sample_id}.sorted.vcf.gz
    """
}





process parse_variant_qc {

    input:
    tuple val(sample_id), path(vcf_file)

    output:
    path("${sample_id}_variant_qc.txt")

    publishDir "${params.result_dir ?: './results'}/${sample_id}", mode: 'copy'

    script:
    """
    set -e

    bcftools query -f '%CHROM\\t%POS\\t[%DP]\\t[%AD]\\n' ${vcf_file} | awk '
    BEGIN {
        OFS = "\\t"
        print "CHROM", "POS", "DP", "AD", "Relative_AD"
    }
    {
        chrom = \$1
        pos = \$2
        dp = \$3 + 0
        ad = \$4 + 0
        if (dp >= 10 && ad / dp >= 0.25)
            print chrom, pos, dp, ad, ad/dp
    }
    ' > ${sample_id}_variant_qc.txt
    """
}





process consensus_generation {
    input:
        path vcf_gz        // Fichier VCF compress√©
        path vcf_tbi       // Index .tbi du fichier VCF
        path ref_genome    // Fichier FASTA du g√©nome de r√©f√©rence
        path low_coverage  // Fichier BED avec les r√©gions √† faible couverture

    output:
        path "${vcf_gz.baseName}_consensus.fasta"  // R√©sultat final

    publishDir "${params.result_dir ?: './results'}", mode: 'copy'

    script:
    """
    # Commande bcftools pour g√©n√©rer la s√©quence consensus
    bcftools consensus \\
        -f ${ref_genome} \\
        -m ${low_coverage} \\
        ${vcf_gz} > ${vcf_gz.baseName}_consensus.fasta
    """
}





process consensus_generation {

    input:
    tuple val(sample_id), path(vcf_file), path(vcf_index)
    path ref_genome
    path lowcov_bed

    output:
    path("${sample_id}_tmp.fa")
    path("${sample_id}_consensus.fasta")

    publishDir "${params.result_dir ?: './results'}/${sample_id}", mode: 'copy'

    script:
    """
    set -e

    # √âtape 1 : Appliquer les variants
    bcftools consensus -f ${ref_genome} ${vcf_file} > ${sample_id}_tmp.fa

    # √âtape 2 : Masquer les r√©gions √† faible couverture
    bedtools maskfasta -fi ${sample_id}_tmp.fa -bed ${lowcov_bed} -fo ${sample_id}_consensus.fasta
    """
}
